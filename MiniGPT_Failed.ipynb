{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQzV3X0k/Gh9W+51TCCgty",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arpit1118/LLM/blob/main/MiniGPT_Failed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BzQeGu6fxsoK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d78f7f8-4993-4b33-9193-e4d50f90421c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 3.6815\n",
            "Epoch 10, Loss: 3.4729\n",
            "Epoch 20, Loss: 3.3066\n",
            "Epoch 30, Loss: 3.1964\n",
            "Epoch 40, Loss: 3.3391\n",
            "Epoch 50, Loss: 3.0885\n",
            "Epoch 60, Loss: 3.1379\n",
            "Epoch 70, Loss: 2.9660\n",
            "Epoch 80, Loss: 2.9482\n",
            "Epoch 90, Loss: 2.9930\n",
            "Epoch 100, Loss: 2.8843\n",
            "Epoch 110, Loss: 2.8407\n",
            "Epoch 120, Loss: 2.8068\n",
            "Epoch 130, Loss: 2.8014\n",
            "Epoch 140, Loss: 2.5859\n",
            "Epoch 150, Loss: 2.6082\n",
            "Epoch 160, Loss: 2.6740\n",
            "Epoch 170, Loss: 2.4984\n",
            "Epoch 180, Loss: 2.5507\n",
            "Epoch 190, Loss: 2.4869\n",
            "Epoch 200, Loss: 2.5444\n",
            "Epoch 210, Loss: 2.4294\n",
            "Epoch 220, Loss: 2.3008\n",
            "Epoch 230, Loss: 2.5340\n",
            "Epoch 240, Loss: 2.3988\n",
            "Epoch 250, Loss: 2.4055\n",
            "Epoch 260, Loss: 2.4118\n",
            "Epoch 270, Loss: 2.2378\n",
            "Epoch 280, Loss: 2.2837\n",
            "Epoch 290, Loss: 2.3419\n",
            "Epoch 300, Loss: 2.3738\n",
            "Epoch 310, Loss: 2.1143\n",
            "Epoch 320, Loss: 2.3671\n",
            "Epoch 330, Loss: 2.0733\n",
            "Epoch 340, Loss: 2.3203\n",
            "Epoch 350, Loss: 2.2965\n",
            "Epoch 360, Loss: 2.1971\n",
            "Epoch 370, Loss: 2.0292\n",
            "Epoch 380, Loss: 2.2302\n",
            "Epoch 390, Loss: 2.2048\n",
            "Tu\"=mS.\n",
            "D.tvaYoy is vayaxl vDp awLY .wwaou_)aob.dasLiYs ttw_ i\n",
            "This\n",
            "vy biYiveiNa_tke.vy kveubgata\n",
            "aT)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Hyperparameters\n",
        "block_size = 8  # context window\n",
        "batch_size = 16\n",
        "embedding_dim = 32\n",
        "num_heads = 2\n",
        "num_layers = 2\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 400\n",
        "\n",
        "# Load text\n",
        "with open(\"tiny_dataset.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Tokenization\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# Train/val split\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    d = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(d) - block_size, (batch_size,))\n",
        "    x = torch.stack([d[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([d[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "# Model\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_embedding = nn.Embedding(block_size, embedding_dim)\n",
        "        self.transformer_blocks = nn.Sequential(*[\n",
        "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, batch_first=True)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        tok_emb = self.token_embedding(x)\n",
        "        pos_emb = self.pos_embedding(torch.arange(T, device=x.device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.transformer_blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n",
        "\n",
        "model = GPT()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    xb, yb = get_batch(\"train\")\n",
        "    logits = model(xb)\n",
        "    loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Text Generation\n",
        "def generate(idx, max_new_tokens=50):\n",
        "    model.eval()\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "        logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "# Start with \"T\"\n",
        "context = torch.tensor([[stoi['T']]], dtype=torch.long)\n",
        "output = generate(context, max_new_tokens=100)\n",
        "print(decode(output[0].tolist()))\n"
      ]
    }
  ]
}